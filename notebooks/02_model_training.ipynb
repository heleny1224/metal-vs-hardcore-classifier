{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd847d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import random, math\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf93ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(gtzan_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.class_names = sorted(dataframe['Genre'].unique())\n",
    "        self.class_to_index = {class_name: i for i, class_name in enumerate(self.class_names)}\n",
    "        self.file_list = [(row['Path'], self.class_to_index[row['Genre']]) for index, row in dataframe.iterrows()]\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.sr = 44100\n",
    "        self.duration = 5500\n",
    "        self.channel = 2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, class_id = self.file_list[idx]\n",
    "        \n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        resample = AudioUtil.resample(aud, self.sr)\n",
    "        rechannel = AudioUtil.rechannel(resample, self.channel)\n",
    "        equal_dur = AudioUtil.pad_trunc(rechannel, self.duration)\n",
    "        melspectrogram = AudioUtil.spectro_gram(equal_dur)\n",
    "\n",
    "        return melspectrogram, class_id\n",
    "        \n",
    "def create_data_loader(audio_folder, max_batch_size=16, shuffle=True):\n",
    "    audio_dataset = AudioDataset(audio_folder)\n",
    "    data_loader = torch.utils.data.DataLoader(audio_dataset, batch_size=max_batch_size, shuffle=shuffle)\n",
    "    return data_loader\n",
    "\n",
    "class_names = gtzan_df['Genre'].unique()\n",
    "print(class_names)\n",
    "\n",
    "train_loader = create_data_loader(train_df)\n",
    "val_loader = create_data_loader(val_df)\n",
    "test_loader = create_data_loader(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_images_from_loader(data_loader, num_images=16):\n",
    "    indices = np.random.choice(len(data_loader.dataset), num_images, replace=False)\n",
    "    \n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        spectrogram, label = data_loader.dataset[idx]\n",
    "        spectrogram_data = spectrogram[0]\n",
    "        spectrograms.append(spectrogram_data)\n",
    "        labels.append(label)\n",
    "\n",
    "    rows = int(np.ceil(num_images / 4))\n",
    "    fig, axes = plt.subplots(rows, 4, figsize=(15, 3 * rows))\n",
    "\n",
    "    for i, (specgram, label) in enumerate(zip(spectrograms, labels)):\n",
    "        ax = axes[i // 4, i % 4] if rows > 1 else axes[i % 4]\n",
    "        plot_spectrogram(specgram, title=f\"Label: {class_names[label]}\", ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_images_from_loader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f687390",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SpectrogramCNN_GRUNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SpectrogramCNN_GRUNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        \n",
    "        self.gru_input_size = 128 * 8\n",
    "        self.gru1 = nn.GRU(input_size=self.gru_input_size, hidden_size=68, batch_first=True, num_layers=1)\n",
    "        self.gru2 = nn.GRU(input_size=68, hidden_size=68, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(68, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.drop_fc = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b, w, c * h)\n",
    "\n",
    "        x, _ = self.gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.drop_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SpectrogramCNN_GRUNet(num_classes=10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "input_tensor = torch.randn(16, 1, 96, 1366).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_iterator = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_iterator):\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_iterator.set_postfix(loss=running_loss / ((batch_idx + 1) * train_loader.batch_size))\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}\")\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'DM_gtzan_best.pth')\n",
    "\n",
    "print(f\"Best validation accuracy: {best_accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
